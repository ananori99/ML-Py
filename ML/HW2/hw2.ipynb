{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sepal.Length  Sepal.Width Species\n",
      "0           5.1          3.5  setosa\n",
      "1           4.9          3.0  setosa\n",
      "2           4.7          3.2  setosa\n",
      "3           4.6          3.1  setosa\n",
      "4           5.0          3.6  setosa\n"
     ]
    }
   ],
   "source": [
    "#The files is “hw2_data_1.txt”.  The data contains two X variables and one Y variable (two classes).\n",
    "#Use rows 1-70 as training data, and use the remaining rows as testing data.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import math as math\n",
    "import scipy as sp\n",
    "from functools import reduce\n",
    "from sklearn import tree\n",
    "import random as nr\n",
    "import itertools as it\n",
    "%matplotlib inline\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ### data path specification here\n",
    "    #use relative path here since the script is in the same directory as the data file\n",
    "    data1 = \"hw2_data_1.txt\"\n",
    "    \n",
    "\n",
    "\n",
    "    ### load the data into pandas dataframe\n",
    "    total_df = pd.read_csv(data1, sep=\"\\t\") #separator is always important; use \",\" for this small dataset\n",
    "\n",
    "    print(total_df.head()) # get a peek at what each column contains\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n",
      "Index(['Sepal.Length', 'Sepal.Width', 'Species'], dtype='object')\n",
      "Sepal.Length    float64\n",
      "Sepal.Width     float64\n",
      "Species          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "    print(total_df.shape) # know the shape of the data\n",
    "    print(total_df.columns) # get the list of column names\n",
    "    print(total_df.dtypes) # know the data types of each column (float, int, object(string) and etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "Y = total_df[\"Species\"].tolist()\n",
    "def lab(s): \n",
    "    if(s == 'setosa'): return 1;\n",
    "    else:return -1;\n",
    "Y= list(map(lab,total_df[\"Species\"].tolist()))\n",
    "print(list(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1]\n",
      " [4.9 3.0 1]\n",
      " [4.7 3.2 1]\n",
      " [4.6 3.1 1]\n",
      " [5.0 3.6 1]\n",
      " [5.4 3.9 1]\n",
      " [4.6 3.4 1]\n",
      " [5.0 3.4 1]\n",
      " [4.4 2.9 1]\n",
      " [4.9 3.1 1]\n",
      " [5.4 3.7 1]\n",
      " [4.8 3.4 1]\n",
      " [4.8 3.0 1]\n",
      " [4.3 3.0 1]\n",
      " [5.8 4.0 1]\n",
      " [5.7 4.4 1]\n",
      " [5.4 3.9 1]\n",
      " [5.1 3.5 1]\n",
      " [5.7 3.8 1]\n",
      " [5.1 3.8 1]\n",
      " [5.4 3.4 1]\n",
      " [5.1 3.7 1]\n",
      " [4.6 3.6 1]\n",
      " [5.1 3.3 1]\n",
      " [4.8 3.4 1]\n",
      " [5.0 3.0 1]\n",
      " [5.0 3.4 1]\n",
      " [5.2 3.5 1]\n",
      " [5.2 3.4 1]\n",
      " [4.7 3.2 1]\n",
      " [4.8 3.1 1]\n",
      " [5.4 3.4 1]\n",
      " [5.2 4.1 1]\n",
      " [5.5 4.2 1]\n",
      " [4.9 3.1 1]\n",
      " [6.3 3.3 -1]\n",
      " [5.8 2.7 -1]\n",
      " [7.1 3.0 -1]\n",
      " [6.3 2.9 -1]\n",
      " [6.5 3.0 -1]\n",
      " [7.6 3.0 -1]\n",
      " [4.9 2.5 -1]\n",
      " [7.3 2.9 -1]\n",
      " [6.7 2.5 -1]\n",
      " [7.2 3.6 -1]\n",
      " [6.5 3.2 -1]\n",
      " [6.4 2.7 -1]\n",
      " [6.8 3.0 -1]\n",
      " [5.7 2.5 -1]\n",
      " [5.8 2.8 -1]\n",
      " [6.4 3.2 -1]\n",
      " [6.5 3.0 -1]\n",
      " [7.7 3.8 -1]\n",
      " [7.7 2.6 -1]\n",
      " [6.0 2.2 -1]\n",
      " [6.9 3.2 -1]\n",
      " [5.6 2.8 -1]\n",
      " [7.7 2.8 -1]\n",
      " [6.3 2.7 -1]\n",
      " [6.7 3.3 -1]\n",
      " [7.2 3.2 -1]\n",
      " [6.2 2.8 -1]\n",
      " [6.1 3.0 -1]\n",
      " [6.4 2.8 -1]\n",
      " [7.2 3.0 -1]\n",
      " [7.4 2.8 -1]\n",
      " [7.9 3.8 -1]\n",
      " [6.4 2.8 -1]\n",
      " [6.3 2.8 -1]\n",
      " [6.1 2.6 -1]]\n"
     ]
    }
   ],
   "source": [
    "# Write a function of perceptron. Using initial weights of all “1”s,\n",
    "#and a learning rate of 1, run the perception on the training data. \n",
    "#Conduct prediction on the testing data, and report error rate. \n",
    "#Please remember to add the column of 1’s in the predictors.\n",
    "#step function for activation\n",
    "def step (x): \n",
    "    if((x >= 1) ): return 1;\n",
    "    else: return -1;\n",
    "\n",
    "\n",
    "#initalize weights, leanning rate, and threshold\n",
    "\n",
    "df = total_df.values\n",
    "dfTr = df[0:70,:]\n",
    "dfTr[:,2] = list(map(lab,dfTr[:,2].tolist()))\n",
    "print(dfTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc(it, lr):\n",
    "    stit = str(it);\n",
    "    w = np.ones([2, 1]) #weight\n",
    "# Apply Perceptron learning rule\n",
    "    for _ in range(it) :  #This is too help converge\n",
    "        for i in range(70): #up to 70\n",
    "            i = nr.randint(0,70); #thought that this would help\n",
    "        #for the training set, calculate output\n",
    "            y = step(0.01*w.transpose().dot(df[i, 0:2]));\n",
    "            # print((Y[i], y))\n",
    "            # Update weights\n",
    "            #if both y are 0, then the update is w + 0\n",
    "            #if both are 1, it is w+(1-1)*...\n",
    "            #else it is w +lr*(x), where x is a missclassified point\n",
    "            #w is thus a linear combination of misclassified points like the lecture 8 slides say\n",
    "            w = w + lr*((Y[i] - y) )* df[i, 0:2].reshape(w.shape[0], 1);\n",
    "           \n",
    "    #now do prediction and errors \n",
    "    err = 0;\n",
    "    for i in range(70,100) :\n",
    "        #print(w.transpose().dot(df[i, 0:2]));\n",
    "        y = step(w.transpose().dot(df[i, 0:2])-60);#bias\n",
    "        err += int(Y[i] != y)\n",
    "\n",
    "    print(\"For \" +stit + \" iterates, the error is \"+ str(err)+\"/30\")\n",
    "    print(\"w is \" + str(w));\n",
    "    return w;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 3 iterates, the error is 12/30\n",
      "w is [[-25.200000000000017]\n",
      " [75.80000000000003]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-25.200000000000017],\n",
       "       [75.80000000000003]], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 5 iterates, the error is 9/30\n",
      "w is [[-27.2]\n",
      " [79.99999999999999]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-27.2],\n",
       "       [79.99999999999999]], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 10 iterates, the error is 7/30\n",
      "w is [[-36.20000000000001]\n",
      " [94.80000000000003]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-36.20000000000001],\n",
       "       [94.80000000000003]], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 20 iterates, the error is 8/30\n",
      "w is [[-40.39999999999998]\n",
      " [105.0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-40.39999999999998],\n",
       "       [105.0]], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc(20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the code for just one desicion stump\n",
    "#input\n",
    "#x is the variables and outcomes to train on\n",
    "#w is the weight to give each sample\n",
    "#output\n",
    "# an array of the two best cuts for each var [dimension 0, dimension1]\n",
    "#the preformance and the dimension that gave the best cut as [pref d1, pref d2]\n",
    "\n",
    "def stump(x,w):\n",
    "    #x is data\n",
    "    #w is weight on data/correct answer\n",
    "    #both for rows and columns\n",
    "    #multiply w and y element wise\n",
    "    x[:,2]= np.multiply(x[:,2],w)\n",
    "    ext=[0,0]\n",
    "    inx = [0,0]\n",
    "    val = [0,0] \n",
    "\n",
    "    for i in range(2):\n",
    "        #sort samples in ascending order along dimension i\n",
    "        x[np.argsort(x[:, i])]#print(i)\n",
    "        #x[np.argsort(x[:, 1])]\n",
    "        #x[x[:,i].argsort( axis=0, order=None)]\n",
    "        #print(list(x))\n",
    "        #as the y labels are +/- 1, do a cumulative sum with the weight\n",
    "        #the array should have each element map to the sum reduction of it's subsequence\n",
    "        ex = list(it.accumulate(x[:,2].tolist()))\n",
    "        #this max will be the most pure in terms of ones to the left of it\n",
    "        print(max(ex),i)\n",
    "        ind = ex.index(max(ex))\n",
    "        ex1 = x[ind,i]\n",
    "        \n",
    "      \n",
    "        if (max(ex)> val[0]):\n",
    "            val[0] = max(ex)\n",
    "            val[1] = i\n",
    "            \n",
    "        #return the info needed to decide the class, ex for sign, ind for cut off, i to know if in x1 or x2 var\n",
    "        ext[i]=ex1\n",
    "        inx[i]=ind\n",
    "        \n",
    "    return [ext, val]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adative boost\n",
    "#input:\n",
    "#n is the number of itterataes/ learnes to make\n",
    "#X is the data with outcomes to train on\n",
    "def adab2( X,n):\n",
    "    #n initail weights for the n learners to have\n",
    "    #the weights are all initialized to be equal, 1/n\n",
    "    w = np.ones(70)/70\n",
    "    #the container for the learners\n",
    "    l = []\n",
    "    #the weight on each learner\n",
    "    lw = np.ones(n)\n",
    "\n",
    "    \n",
    "    #for the number of epochs, train a classifier with the weight adjusting at each iteration\n",
    "    for i in range(n):\n",
    "        #train a classifier on the weigthed data\n",
    "        s = stump(X,w)\n",
    "        #this stump gives two lists: where to cut and the value of that cut, the index corresponds to the index of that \n",
    "        #dimension\n",
    "        #find out if the x1 or x2 cut is more valuable\n",
    "        ind = s[1].index(max(s[1]))\n",
    "        s2 = [ind, s[0][ind] ]\n",
    "        \n",
    "        #then predict on x\n",
    "        yp = list(map(lambda h:step(int(h)),(X[:,ind] <= s[0][ind])))\n",
    "        y = X[:,2].astype('int')\n",
    "        \n",
    "        #find the error of that classifier and use it for the updates\n",
    "        #weighted sum error for misclassified points\n",
    "        e = w.dot(yp != y)\n",
    "        #use that error to find the voting weight of this stump\n",
    "        a = (np.log((1 - e)/e) / 2)\n",
    "        lw[i] = a\n",
    "        l.append(s2)\n",
    "        \n",
    "        #find the new weights of the samples\n",
    "        w = np.asarray(w) * np.exp(- a * y * yp)\n",
    "        w = w / w.sum()\n",
    "        lw = lw/lw.sum()\n",
    "\n",
    "    return [lw,l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adative boost\n",
    "#input:\n",
    "#n is the number of itterataes/ learnes to make\n",
    "#X is the data with outcomes to train on\n",
    "def adab( X,n):\n",
    "    #n initail weights for the n learners to have\n",
    "    #the weights are all initialized to be equal, 1/n\n",
    "    w = np.ones(70)/70\n",
    "    #the container for the learners\n",
    "    l = []\n",
    "    #the weight on each learner\n",
    "    lw = np.ones(n)\n",
    "\n",
    "    \n",
    "    #for the number of epochs, train a classifier with the weight adjusting at each iteration\n",
    "    for i in range(n):\n",
    "        #train a classifier on the weigthed data\n",
    "        s = tree.DecisionTreeClassifier(max_depth=1)\n",
    "        s.fit(X[:,0:1], X[:,2].astype('int'), sample_weight=w)\n",
    "        yp = s.predict(X[:,0:1])\n",
    "        y = X[:,2].astype('int')\n",
    "        \n",
    "        #find the error of that classifier and use it for the updates\n",
    "        #weighted sum error for misclassified points\n",
    "        e = w.dot(yp != y)\n",
    "        #use that error to find the voting weight of this stump\n",
    "        a = (np.log((1 - e)/e) / 2)\n",
    "        lw[i] = a\n",
    "        l.append(s)\n",
    "        \n",
    "        #find the new weights of the samples\n",
    "        w = np.asarray(w) * np.exp(- a * y * yp)\n",
    "        w = w / w.sum()\n",
    "        lw = lw/lw.sum()\n",
    "\n",
    "    return [lw,l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction's confidence (abs) are  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.6348449080299385, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.6348449080299385]\n",
      "predicted class  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "true class  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "error rate is  0.0\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "#predict and report error\n",
    "t = adab(df[0:70, :],3)\n",
    "\n",
    "#predict on the test set\n",
    "# X input, y output\n",
    "y = np.zeros(30)\n",
    "for (s, w) in zip(t[1], t[0]):\n",
    "    y = y + w * s.predict(df[70:100, 0:1])\n",
    "print(\"the prediction's confidence (abs) are \", list(y))\n",
    "y = list(map(lambda j:step(int(j>0)),y))\n",
    "print(\"predicted class \",list(y))\n",
    "yt = Y[70:100]\n",
    "print(\"true class \",yt)\n",
    "#see where the values are not equal\n",
    "e = reduce((lambda i,j: i+j), list(map(lambda x, z: int(x != z),y , yt)) )/30\n",
    "print(\"error rate is \",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction's confidence (abs) are  [0.45134848816932016, 0.45134848816932016, 0.45134848816932016, 0.45134848816932016, 0.45134848816932016, 0.45134848816932016, 0.45134848816932016, 0.45134848816932016, 0.45134848816932016, 0.45134848816932016, 0.45134848816932016, 0.45134848816932016, 0.45134848816932016, 0.45134848816932016, 0.45134848816932016, -0.45134848816932016, -0.45134848816932016, -0.45134848816932016, -0.45134848816932016, -0.45134848816932016, -0.45134848816932016, -0.45134848816932016, 0.22407604972873826, -0.45134848816932016, -0.45134848816932016, -0.45134848816932016, -0.45134848816932016, -0.45134848816932016, -0.45134848816932016, 0.22407604972873826] \n",
      "\n",
      "predicted class  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1] \n",
      "\n",
      "true class  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "error rate is  0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "#predict and report error\n",
    "t = adab(df[0:70, :],5)\n",
    "\n",
    "#predict on the test set\n",
    "# X input, y output\n",
    "y = np.zeros(30)\n",
    "for (s, w) in zip(t[1], t[0]):\n",
    "    y = y + w * s.predict(df[70:100, 0:1])\n",
    "print(\"the prediction's confidence (abs) are \", list(y),\"\\n\")\n",
    "y = list(map(lambda j:step(int(j>0)),y))\n",
    "print(\"predicted class \",list(y), \"\\n\")\n",
    "yt = Y[70:100]\n",
    "print(\"true class \",yt)\n",
    "#see where the values are not equal\n",
    "e = reduce((lambda i,j: i+j), list(map(lambda x, z: int(x != z),y , yt)) )/30\n",
    "print(\"error rate is \",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction's confidence (abs) are  [0.16653450999474034, 0.16653450999474034, -0.08657114163958951, 0.6246200265953243, 0.16653450999474034, 0.16653450999474034, 0.6246200265953243, 0.6246200265953243, 0.16653450999474034, 0.16653450999474034, 0.6246200265953243, 0.16653450999474034, 0.6246200265953243, 0.16653450999474034, 0.16653450999474034, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, 0.16189269365757392, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, 0.16189269365757392] \n",
      "\n",
      "predicted class  [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1] \n",
      "\n",
      "true class  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "error rate is  0.1\n"
     ]
    }
   ],
   "source": [
    "#10, 20\n",
    "#predict and report error\n",
    "t = adab(df[0:70, :],10)\n",
    "\n",
    "#predict on the test set\n",
    "# X input, y output\n",
    "y = np.zeros(30)\n",
    "for (s, w) in zip(t[1], t[0]):\n",
    "    y = y + w * s.predict(df[70:100, 0:1])\n",
    "print(\"the prediction's confidence (abs) are \", list(y),\"\\n\")\n",
    "y = list(map(lambda j:step(int(j>0)),y))\n",
    "print(\"predicted class \",list(y), \"\\n\")\n",
    "yt = Y[70:100]\n",
    "print(\"true class \",yt)\n",
    "#see where the values are not equal\n",
    "e = reduce((lambda i,j: i+j), list(map(lambda x, z: int(x != z),y , yt)) )/30\n",
    "print(\"error rate is \",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction's confidence (abs) are  [0.17691745173094958, 0.17691745173094958, -0.11769271071829987, 0.34740210853384673, 0.17691745173094958, 0.17691745173094958, 0.34740210853384673, 0.34740210853384673, 0.17691745173094958, 0.17691745173094958, 0.34740210853384673, 0.17691745173094958, 0.34740210853384673, 0.17691745173094958, 0.17691745173094958, -0.5572407846287333, -0.5572407846287333, -0.5572407846287333, -0.5572407846287333, -0.5572407846287333, -0.5572407846287333, -0.5572407846287333, 0.12825345084393705, -0.5572407846287333, -0.5572407846287333, -0.5572407846287333, -0.5572407846287333, -0.5572407846287333, -0.5572407846287333, 0.12825345084393705] \n",
      "\n",
      "predicted class  [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1] \n",
      "\n",
      "true class  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "error rate is  0.1\n"
     ]
    }
   ],
   "source": [
    "#20\n",
    "#predict and report error\n",
    "t = adab(df[0:70, :],20)\n",
    "\n",
    "#predict on the test set\n",
    "# X input, y output\n",
    "y = np.zeros(30)\n",
    "for (s, w) in zip(t[1], t[0]):\n",
    "    y = y + w * s.predict(df[70:100, 0:1])\n",
    "print(\"the prediction's confidence (abs) are \", list(y),\"\\n\")\n",
    "y = list(map(lambda j:step(int(j>0)),y))\n",
    "print(\"predicted class \",list(y), \"\\n\")\n",
    "yt = Y[70:100]\n",
    "print(\"true class \",yt)\n",
    "#see where the values are not equal\n",
    "e = reduce((lambda i,j: i+j), list(map(lambda x, z: int(x != z),y , yt)) )/30\n",
    "print(\"error rate is \",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction's confidence (abs) are  [0.16653450999474034, 0.16653450999474034, -0.08657114163958951, 0.6246200265953243, 0.16653450999474034, 0.16653450999474034, 0.6246200265953243, 0.6246200265953243, 0.16653450999474034, 0.16653450999474034, 0.6246200265953243, 0.16653450999474034, 0.6246200265953243, 0.16653450999474034, 0.16653450999474034, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, 0.16189269365757392, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, -0.6246200265953243, 0.16189269365757392] \n",
      "\n",
      "predicted class  [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1] \n",
      "\n",
      "true class  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "error rate is  0.1\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "#predict and report error\n",
    "t = adab(df[0:70, :],10)\n",
    "\n",
    "#predict on the test set\n",
    "# X input, y output\n",
    "y = np.zeros(30)\n",
    "for (s, w) in zip(t[1], t[0]):\n",
    "    y = y + w * s.predict(df[70:100, 0:1])\n",
    "print(\"the prediction's confidence (abs) are \", list(y),\"\\n\")\n",
    "y = list(map(lambda j:step(int(j>0)),y))\n",
    "print(\"predicted class \",list(y), \"\\n\")\n",
    "yt = Y[70:100]\n",
    "print(\"true class \",yt)\n",
    "#see where the values are not equal\n",
    "e = reduce((lambda i,j: i+j), list(map(lambda x, z: int(x != z),y , yt)) )/30\n",
    "print(\"error rate is \",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X1    X2    X3     X4    X5     X6    X7    X8    X9   X10  ...    X17  \\\n",
      "0  2.13  3.76  4.93  2.030  6.37  1.260  8.45  2.58  6.57  2.65  ...  1.310   \n",
      "1  3.40  3.33  4.19  0.816  6.80  0.236  7.45  4.33  4.66  1.57  ...  2.580   \n",
      "2  1.17  4.30  4.25  1.640  4.68 -2.100  6.40  4.04  4.11  3.03  ...  2.850   \n",
      "3  2.63  2.97  5.79  1.140  5.14 -0.351  6.89  4.69  5.31  3.50  ...  2.330   \n",
      "4  3.49  1.11  4.99  0.232  4.99 -0.486  8.65  4.45  6.04  2.25  ...  0.931   \n",
      "\n",
      "    X18   X19   X20   X21   X22   X23   X24   X25  y  \n",
      "0  2.77  3.97  7.36  4.99  6.97  5.89  7.04  6.04  1  \n",
      "1  3.66  4.46  6.19  3.58  7.71  6.38  7.22  4.41  1  \n",
      "2  4.09  3.77  5.91  4.37  7.66  6.45  7.28  5.14  1  \n",
      "3  3.38  4.27  6.36  5.70  7.76  3.59  7.34  5.86  1  \n",
      "4  4.39  4.91  7.72  6.71  5.74  7.08  8.15  3.00  0  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "#then for part 2, load data 2\n",
    "data2 = \"hw2_data_2.txt\"\n",
    "    \n",
    "\n",
    "\n",
    "### load the data into pandas dataframe\n",
    "total_df2 = pd.read_csv(data2, sep=\"\\t\") #separator is always important; use \",\" for this small dataset\n",
    "\n",
    "print(total_df2.head()) # get a peek at what each column contains\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 26)\n",
      "Index(['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11',\n",
      "       'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21',\n",
      "       'X22', 'X23', 'X24', 'X25', 'y'],\n",
      "      dtype='object')\n",
      "X1     float64\n",
      "X2     float64\n",
      "X3     float64\n",
      "X4     float64\n",
      "X5     float64\n",
      "X6     float64\n",
      "X7     float64\n",
      "X8     float64\n",
      "X9     float64\n",
      "X10    float64\n",
      "X11    float64\n",
      "X12    float64\n",
      "X13    float64\n",
      "X14    float64\n",
      "X15    float64\n",
      "X16    float64\n",
      "X17    float64\n",
      "X18    float64\n",
      "X19    float64\n",
      "X20    float64\n",
      "X21    float64\n",
      "X22    float64\n",
      "X23    float64\n",
      "X24    float64\n",
      "X25    float64\n",
      "y        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(total_df2.shape) # know the shape of the data\n",
    "print(total_df2.columns) # get the list of column names\n",
    "print(total_df2.dtypes) # know the data types of each column (float, int, object(string) and etc.)\n",
    "\n",
    "df2 = total_df2.values\n",
    "X = df2[0:600,0:25]\n",
    "y = df2[0:600, 25]\n",
    "\n",
    "tX = df2[600:800,0:25]\n",
    "ty = df2[600:800, 25]\n",
    "#test error to rank preformance\n",
    "te = [ 0, 0, 0, 0]\n",
    "ln = ['radial', 'sigmoid', 'polynomial', 'rand forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.02497771, 0.02248666, 0.02247927, 0.02305975, 0.02205021,\n",
      "       0.02211342, 0.0217042 , 0.01404774, 0.02428908, 0.02531197,\n",
      "       0.02671938, 0.01849201, 0.01865923]), 'std_fit_time': array([4.00167247e-03, 4.19050675e-04, 8.27527839e-04, 1.57719855e-03,\n",
      "       5.60791769e-05, 1.12000827e-04, 1.65474279e-03, 2.97041635e-04,\n",
      "       1.61940624e-04, 1.17288200e-04, 9.41153588e-04, 4.01609639e-05,\n",
      "       4.45286016e-04]), 'mean_score_time': array([0.00259125, 0.00232606, 0.00232992, 0.00247629, 0.00229366,\n",
      "       0.00229959, 0.00232782, 0.00148311, 0.00219636, 0.002333  ,\n",
      "       0.00270193, 0.00194495, 0.00199201]), 'std_score_time': array([3.51832565e-04, 6.91221309e-05, 8.15160200e-05, 2.77278082e-04,\n",
      "       3.62539028e-05, 5.48136499e-05, 1.68071611e-04, 2.92678981e-05,\n",
      "       7.07589069e-05, 3.31403626e-05, 2.18006096e-04, 2.12090848e-05,\n",
      "       1.39278594e-04]), 'param_gamma': masked_array(data=[1e-09, 1e-08, 1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.01,\n",
      "                   0.1, 1.0, 10.0, 100.0, 1000.0],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_kernel': masked_array(data=['rbf', 'rbf', 'rbf', 'rbf', 'rbf', 'rbf', 'rbf', 'rbf',\n",
      "                   'rbf', 'rbf', 'rbf', 'rbf', 'rbf'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'gamma': 1e-09, 'kernel': 'rbf'}, {'gamma': 1e-08, 'kernel': 'rbf'}, {'gamma': 1e-07, 'kernel': 'rbf'}, {'gamma': 1e-06, 'kernel': 'rbf'}, {'gamma': 1e-05, 'kernel': 'rbf'}, {'gamma': 0.0001, 'kernel': 'rbf'}, {'gamma': 0.001, 'kernel': 'rbf'}, {'gamma': 0.01, 'kernel': 'rbf'}, {'gamma': 0.1, 'kernel': 'rbf'}, {'gamma': 1.0, 'kernel': 'rbf'}, {'gamma': 10.0, 'kernel': 'rbf'}, {'gamma': 100.0, 'kernel': 'rbf'}, {'gamma': 1000.0, 'kernel': 'rbf'}], 'split0_test_score': array([0.50819672, 0.50819672, 0.50819672, 0.50819672, 0.50819672,\n",
      "       0.50819672, 0.95081967, 0.98360656, 0.90163934, 0.50819672,\n",
      "       0.50819672, 0.50819672, 0.50819672]), 'split1_test_score': array([0.50819672, 0.50819672, 0.50819672, 0.50819672, 0.50819672,\n",
      "       0.50819672, 0.90163934, 0.83606557, 0.83606557, 0.50819672,\n",
      "       0.50819672, 0.50819672, 0.50819672]), 'split2_test_score': array([0.50819672, 0.50819672, 0.50819672, 0.50819672, 0.50819672,\n",
      "       0.50819672, 0.8852459 , 0.8852459 , 0.83606557, 0.50819672,\n",
      "       0.50819672, 0.50819672, 0.50819672]), 'split3_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       , 0.9       , 0.88333333, 0.86666667, 0.5       ,\n",
      "       0.5       , 0.5       , 0.5       ]), 'split4_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       , 0.86666667, 0.96666667, 0.88333333, 0.5       ,\n",
      "       0.5       , 0.5       , 0.5       ]), 'split5_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       , 0.86666667, 0.95      , 0.91666667, 0.5       ,\n",
      "       0.5       , 0.5       , 0.5       ]), 'split6_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       , 0.91666667, 0.93333333, 0.91666667, 0.5       ,\n",
      "       0.5       , 0.5       , 0.5       ]), 'split7_test_score': array([0.50847458, 0.50847458, 0.50847458, 0.50847458, 0.50847458,\n",
      "       0.50847458, 0.88135593, 0.89830508, 0.93220339, 0.50847458,\n",
      "       0.50847458, 0.50847458, 0.50847458]), 'split8_test_score': array([0.50847458, 0.50847458, 0.50847458, 0.50847458, 0.50847458,\n",
      "       0.50847458, 0.89830508, 0.93220339, 0.88135593, 0.50847458,\n",
      "       0.50847458, 0.50847458, 0.50847458]), 'split9_test_score': array([0.50847458, 0.50847458, 0.50847458, 0.50847458, 0.50847458,\n",
      "       0.50847458, 0.91525424, 0.89830508, 0.91525424, 0.50847458,\n",
      "       0.50847458, 0.50847458, 0.50847458]), 'mean_test_score': array([0.505     , 0.505     , 0.505     , 0.505     , 0.505     ,\n",
      "       0.505     , 0.89833333, 0.91666667, 0.88833333, 0.505     ,\n",
      "       0.505     , 0.505     , 0.505     ]), 'std_test_score': array([0.0040839 , 0.0040839 , 0.0040839 , 0.0040839 , 0.0040839 ,\n",
      "       0.0040839 , 0.02428044, 0.04250245, 0.03238508, 0.0040839 ,\n",
      "       0.0040839 , 0.0040839 , 0.0040839 ]), 'rank_test_score': array([4, 4, 4, 4, 4, 4, 2, 1, 3, 4, 4, 4, 4], dtype=int32), 'split0_train_score': array([0.50463822, 0.50463822, 0.50463822, 0.50463822, 0.50463822,\n",
      "       0.50463822, 0.92022263, 0.9554731 , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split1_train_score': array([0.50463822, 0.50463822, 0.50463822, 0.50463822, 0.50463822,\n",
      "       0.50463822, 0.93692022, 0.95361781, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split2_train_score': array([0.50463822, 0.50463822, 0.50463822, 0.50463822, 0.50463822,\n",
      "       0.50463822, 0.92207792, 0.95918367, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split3_train_score': array([0.50555556, 0.50555556, 0.50555556, 0.50555556, 0.50555556,\n",
      "       0.50555556, 0.91851852, 0.94814815, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split4_train_score': array([0.50555556, 0.50555556, 0.50555556, 0.50555556, 0.50555556,\n",
      "       0.50555556, 0.93333333, 0.9537037 , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split5_train_score': array([0.50555556, 0.50555556, 0.50555556, 0.50555556, 0.50555556,\n",
      "       0.50555556, 0.92962963, 0.95740741, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split6_train_score': array([0.50555556, 0.50555556, 0.50555556, 0.50555556, 0.50555556,\n",
      "       0.50555556, 0.92222222, 0.9537037 , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split7_train_score': array([0.50462107, 0.50462107, 0.50462107, 0.50462107, 0.50462107,\n",
      "       0.50462107, 0.93530499, 0.95563771, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split8_train_score': array([0.50462107, 0.50462107, 0.50462107, 0.50462107, 0.50462107,\n",
      "       0.50462107, 0.9168207 , 0.95194085, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split9_train_score': array([0.50462107, 0.50462107, 0.50462107, 0.50462107, 0.50462107,\n",
      "       0.50462107, 0.93160813, 0.96118299, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'mean_train_score': array([0.50500001, 0.50500001, 0.50500001, 0.50500001, 0.50500001,\n",
      "       0.50500001, 0.92666583, 0.95499991, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'std_train_score': array([0.00045365, 0.00045365, 0.00045365, 0.00045365, 0.00045365,\n",
      "       0.00045365, 0.00709273, 0.00351761, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        ])}\n",
      "the error rate for the gamma  [1e-09, 1e-08, 1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] is:  [0.495, 0.495, 0.495, 0.495, 0.495, 0.495, 0.10166666666666668, 0.08333333333333337, 0.11166666666666669, 0.495, 0.495, 0.495, 0.495]\n",
      "the best is  {'gamma': 0.01, 'kernel': 'rbf'}\n",
      "radial-10cv has a test error rate of  0.07999999999999996\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#question 3\n",
    "#Run Support Vector Machine with three different kernels: Radial, polynomial, and sigmoid. \n",
    "#Keep every other parameter default, except the following:\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#For radial kernel, run a grid search for the best gamma parameter.\n",
    "#radial svm\n",
    "# Create a SVC classifier using an RBF kernel\n",
    "#range of gamma to search\n",
    "g = np.logspace(-9, 3, 13)\n",
    "#training with 10 fold cv\n",
    "param_grid = {'gamma':g, 'kernel':['rbf']}\n",
    "rsvm = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=10)\n",
    "rsvm.fit(X, y)\n",
    "\n",
    "#Plot the cross-validation error rate v.s. gamma parameters.\n",
    "res = rsvm.cv_results_\n",
    "print(res)\n",
    "re = list(map(lambda x:float(1.0 -x), res['mean_test_score']))\n",
    "rg = list(range(13)) #\n",
    "tg = list(map(lambda x: float(x), res['param_gamma']))\n",
    "plot = plt.scatter(rg,re)\n",
    "print('the error rate for the gamma ', tg, 'is: ', re)\n",
    "\n",
    "\n",
    " \n",
    "#Fit the final model using the selected gamma, and conduct prediction on the testing data. \n",
    "print('the best is ', rsvm.best_params_)\n",
    "pred = 1 - rsvm.score(tX,ty)\n",
    "#Report testing error rate. \n",
    "print('radial-10cv has a test error rate of ', pred)\n",
    "te[0] = pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.02184739, 0.02049401, 0.02039363, 0.02057948, 0.02065523,\n",
      "       0.02035496, 0.02538843, 0.02558558, 0.01210611, 0.0124285 ,\n",
      "       0.01201367, 0.01208632, 0.01210067]), 'std_fit_time': array([1.24109779e-03, 3.18065709e-04, 8.81599855e-05, 3.98483361e-04,\n",
      "       7.73741369e-04, 9.92707197e-05, 1.29686374e-04, 2.21270966e-04,\n",
      "       3.93240423e-05, 5.81336452e-04, 1.80030594e-04, 5.64815627e-05,\n",
      "       3.84722860e-05]), 'mean_score_time': array([0.0025548 , 0.00226765, 0.00227585, 0.00228786, 0.00227704,\n",
      "       0.0022589 , 0.00279534, 0.00284441, 0.00143816, 0.00151136,\n",
      "       0.00145512, 0.001443  , 0.00147874]), 'std_score_time': array([3.20722268e-04, 5.05387555e-05, 6.54272190e-05, 6.62370410e-05,\n",
      "       4.91865330e-05, 2.42426680e-05, 6.53039978e-05, 2.76421744e-05,\n",
      "       1.90103350e-05, 8.26473422e-05, 5.28527046e-05, 2.10549930e-05,\n",
      "       1.22874610e-04]), 'param_gamma': masked_array(data=[1e-09, 1e-08, 1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.01,\n",
      "                   0.1, 1.0, 10.0, 100.0, 1000.0],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_kernel': masked_array(data=['sigmoid', 'sigmoid', 'sigmoid', 'sigmoid', 'sigmoid',\n",
      "                   'sigmoid', 'sigmoid', 'sigmoid', 'sigmoid', 'sigmoid',\n",
      "                   'sigmoid', 'sigmoid', 'sigmoid'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'gamma': 1e-09, 'kernel': 'sigmoid'}, {'gamma': 1e-08, 'kernel': 'sigmoid'}, {'gamma': 1e-07, 'kernel': 'sigmoid'}, {'gamma': 1e-06, 'kernel': 'sigmoid'}, {'gamma': 1e-05, 'kernel': 'sigmoid'}, {'gamma': 0.0001, 'kernel': 'sigmoid'}, {'gamma': 0.001, 'kernel': 'sigmoid'}, {'gamma': 0.01, 'kernel': 'sigmoid'}, {'gamma': 0.1, 'kernel': 'sigmoid'}, {'gamma': 1.0, 'kernel': 'sigmoid'}, {'gamma': 10.0, 'kernel': 'sigmoid'}, {'gamma': 100.0, 'kernel': 'sigmoid'}, {'gamma': 1000.0, 'kernel': 'sigmoid'}], 'split0_test_score': array([0.50819672, 0.50819672, 0.50819672, 0.50819672, 0.50819672,\n",
      "       0.50819672, 0.6557377 , 0.50819672, 0.50819672, 0.50819672,\n",
      "       0.50819672, 0.50819672, 0.50819672]), 'split1_test_score': array([0.50819672, 0.50819672, 0.50819672, 0.50819672, 0.50819672,\n",
      "       0.50819672, 0.62295082, 0.50819672, 0.50819672, 0.50819672,\n",
      "       0.50819672, 0.50819672, 0.50819672]), 'split2_test_score': array([0.50819672, 0.50819672, 0.50819672, 0.50819672, 0.50819672,\n",
      "       0.50819672, 0.57377049, 0.50819672, 0.50819672, 0.50819672,\n",
      "       0.50819672, 0.50819672, 0.50819672]), 'split3_test_score': array([0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.55, 0.5 , 0.5 , 0.5 , 0.5 ,\n",
      "       0.5 , 0.5 ]), 'split4_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       , 0.53333333, 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       , 0.5       , 0.5       ]), 'split5_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       , 0.53333333, 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       , 0.5       , 0.5       ]), 'split6_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       , 0.51666667, 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       , 0.5       , 0.5       ]), 'split7_test_score': array([0.50847458, 0.50847458, 0.50847458, 0.50847458, 0.50847458,\n",
      "       0.50847458, 0.62711864, 0.50847458, 0.50847458, 0.50847458,\n",
      "       0.50847458, 0.50847458, 0.50847458]), 'split8_test_score': array([0.50847458, 0.50847458, 0.50847458, 0.50847458, 0.50847458,\n",
      "       0.50847458, 0.57627119, 0.50847458, 0.50847458, 0.50847458,\n",
      "       0.50847458, 0.50847458, 0.50847458]), 'split9_test_score': array([0.50847458, 0.50847458, 0.50847458, 0.50847458, 0.50847458,\n",
      "       0.50847458, 0.57627119, 0.50847458, 0.50847458, 0.50847458,\n",
      "       0.50847458, 0.50847458, 0.50847458]), 'mean_test_score': array([0.505     , 0.505     , 0.505     , 0.505     , 0.505     ,\n",
      "       0.505     , 0.57666667, 0.505     , 0.505     , 0.505     ,\n",
      "       0.505     , 0.505     , 0.505     ]), 'std_test_score': array([0.0040839 , 0.0040839 , 0.0040839 , 0.0040839 , 0.0040839 ,\n",
      "       0.0040839 , 0.04371823, 0.0040839 , 0.0040839 , 0.0040839 ,\n",
      "       0.0040839 , 0.0040839 , 0.0040839 ]), 'rank_test_score': array([2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2], dtype=int32), 'split0_train_score': array([0.50463822, 0.50463822, 0.50463822, 0.50463822, 0.50463822,\n",
      "       0.50463822, 0.58627087, 0.50463822, 0.50463822, 0.50463822,\n",
      "       0.50463822, 0.50463822, 0.50463822]), 'split1_train_score': array([0.50463822, 0.50463822, 0.50463822, 0.50463822, 0.50463822,\n",
      "       0.50463822, 0.60853432, 0.50463822, 0.50463822, 0.50463822,\n",
      "       0.50463822, 0.50463822, 0.50463822]), 'split2_train_score': array([0.50463822, 0.50463822, 0.50463822, 0.50463822, 0.50463822,\n",
      "       0.50463822, 0.567718  , 0.50463822, 0.50463822, 0.50463822,\n",
      "       0.50463822, 0.50463822, 0.50463822]), 'split3_train_score': array([0.50555556, 0.50555556, 0.50555556, 0.50555556, 0.50555556,\n",
      "       0.50555556, 0.56851852, 0.50555556, 0.50555556, 0.50555556,\n",
      "       0.50555556, 0.50555556, 0.50555556]), 'split4_train_score': array([0.50555556, 0.50555556, 0.50555556, 0.50555556, 0.50555556,\n",
      "       0.50555556, 0.58148148, 0.50555556, 0.50555556, 0.50555556,\n",
      "       0.50555556, 0.50555556, 0.50555556]), 'split5_train_score': array([0.50555556, 0.50555556, 0.50555556, 0.50555556, 0.50555556,\n",
      "       0.50555556, 0.58148148, 0.50555556, 0.50555556, 0.50555556,\n",
      "       0.50555556, 0.50555556, 0.50555556]), 'split6_train_score': array([0.50555556, 0.50555556, 0.50555556, 0.50555556, 0.50555556,\n",
      "       0.50555556, 0.60925926, 0.50555556, 0.50555556, 0.50555556,\n",
      "       0.50555556, 0.50555556, 0.50555556]), 'split7_train_score': array([0.50462107, 0.50462107, 0.50462107, 0.50462107, 0.50462107,\n",
      "       0.50462107, 0.60813309, 0.50462107, 0.50462107, 0.50462107,\n",
      "       0.50462107, 0.50462107, 0.50462107]), 'split8_train_score': array([0.50462107, 0.50462107, 0.50462107, 0.50462107, 0.50462107,\n",
      "       0.50462107, 0.60628466, 0.50462107, 0.50462107, 0.50462107,\n",
      "       0.50462107, 0.50462107, 0.50462107]), 'split9_train_score': array([0.50462107, 0.50462107, 0.50462107, 0.50462107, 0.50462107,\n",
      "       0.50462107, 0.58595194, 0.50462107, 0.50462107, 0.50462107,\n",
      "       0.50462107, 0.50462107, 0.50462107]), 'mean_train_score': array([0.50500001, 0.50500001, 0.50500001, 0.50500001, 0.50500001,\n",
      "       0.50500001, 0.59036336, 0.50500001, 0.50500001, 0.50500001,\n",
      "       0.50500001, 0.50500001, 0.50500001]), 'std_train_score': array([0.00045365, 0.00045365, 0.00045365, 0.00045365, 0.00045365,\n",
      "       0.00045365, 0.01562205, 0.00045365, 0.00045365, 0.00045365,\n",
      "       0.00045365, 0.00045365, 0.00045365])}\n",
      "the error rate for the gamma  [1e-09, 1e-08, 1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] is:  [0.495, 0.495, 0.495, 0.495, 0.495, 0.495, 0.42333333333333334, 0.495, 0.495, 0.495, 0.495, 0.495, 0.495]\n",
      "the best is  {'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "sigmoid-10cv has a test error rate of  0.385\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#For sigmoid kernel, conduct the same procedure as for the radial kernel. \n",
    "#Use 10-fold cross-validation to select the best gamma parameter. \n",
    "#Plot the cross-validation error rate v.s. gamma parameters. \n",
    "#Fit the final model using the selected gamma, and conduct prediction on the testing data. \n",
    "#Report testing error rate. \n",
    "\n",
    "\n",
    "\n",
    "g = np.logspace(-9, 3, 13)\n",
    "#training with 10 fold cv\n",
    "param_grid = {'gamma':g, 'kernel':['sigmoid']}\n",
    "rsvm = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=10)\n",
    "rsvm.fit(X, y)\n",
    "\n",
    "#Plot the cross-validation error rate v.s. gamma parameters.\n",
    "res = rsvm.cv_results_\n",
    "print(res)\n",
    "re = list(map(lambda x:float(1.0 -x), res['mean_test_score']))\n",
    "rg = list(range(13)) #\n",
    "tg = list(map(lambda x: float(x), res['param_gamma']))\n",
    "plot = plt.scatter(rg,re)\n",
    "print('the error rate for the gamma ', tg, 'is: ', re)\n",
    "\n",
    "\n",
    " \n",
    "#Fit the final model using the selected gamma, and conduct prediction on the testing data. \n",
    "print('the best is ', rsvm.best_params_)\n",
    "pred = 1 - rsvm.score(tX,ty)\n",
    "#Report testing error rate. \n",
    "print('sigmoid-10cv has a test error rate of ', pred)\n",
    "te[1] = pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.01291113, 0.00767145, 0.01601577, 0.02798727, 0.01992438,\n",
      "       0.01786034, 0.01683226, 0.01517081, 0.01388986, 0.01309428,\n",
      "       0.01292491, 0.0135159 , 0.0122787 ]), 'std_fit_time': array([0.00074381, 0.00027517, 0.00105193, 0.00341477, 0.00125348,\n",
      "       0.00084731, 0.00140471, 0.00093557, 0.00087198, 0.00084281,\n",
      "       0.00109664, 0.00114951, 0.00094636]), 'mean_score_time': array([0.00163679, 0.00080621, 0.00059686, 0.0007921 , 0.0006182 ,\n",
      "       0.0006578 , 0.00068369, 0.00069292, 0.00066977, 0.00071619,\n",
      "       0.00069156, 0.00080874, 0.00075104]), 'std_score_time': array([1.98194134e-04, 2.47136125e-05, 6.98637389e-05, 1.50179977e-04,\n",
      "       6.09417150e-05, 8.22792845e-05, 6.17188480e-05, 7.81027424e-05,\n",
      "       6.93992050e-05, 1.64068450e-04, 6.37084780e-05, 1.04474872e-04,\n",
      "       1.25475621e-04]), 'param_degree': masked_array(data=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_kernel': masked_array(data=['poly', 'poly', 'poly', 'poly', 'poly', 'poly', 'poly',\n",
      "                   'poly', 'poly', 'poly', 'poly', 'poly', 'poly'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'degree': 0, 'kernel': 'poly'}, {'degree': 1, 'kernel': 'poly'}, {'degree': 2, 'kernel': 'poly'}, {'degree': 3, 'kernel': 'poly'}, {'degree': 4, 'kernel': 'poly'}, {'degree': 5, 'kernel': 'poly'}, {'degree': 6, 'kernel': 'poly'}, {'degree': 7, 'kernel': 'poly'}, {'degree': 8, 'kernel': 'poly'}, {'degree': 9, 'kernel': 'poly'}, {'degree': 10, 'kernel': 'poly'}, {'degree': 11, 'kernel': 'poly'}, {'degree': 12, 'kernel': 'poly'}], 'split0_test_score': array([0.50819672, 0.98360656, 0.96721311, 0.96721311, 0.96721311,\n",
      "       0.96721311, 0.96721311, 0.96721311, 0.96721311, 0.96721311,\n",
      "       0.96721311, 0.98360656, 1.        ]), 'split1_test_score': array([0.50819672, 0.90163934, 0.8852459 , 0.8852459 , 0.8852459 ,\n",
      "       0.8852459 , 0.8852459 , 0.8852459 , 0.8852459 , 0.8852459 ,\n",
      "       0.8852459 , 0.90163934, 0.8852459 ]), 'split2_test_score': array([0.50819672, 0.90163934, 0.93442623, 0.86885246, 0.86885246,\n",
      "       0.85245902, 0.85245902, 0.85245902, 0.85245902, 0.85245902,\n",
      "       0.85245902, 0.83606557, 0.81967213]), 'split3_test_score': array([0.5       , 0.93333333, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.93333333, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.91666667, 0.91666667]), 'split4_test_score': array([0.5       , 0.93333333, 0.96666667, 0.93333333, 0.91666667,\n",
      "       0.91666667, 0.91666667, 0.9       , 0.88333333, 0.88333333,\n",
      "       0.88333333, 0.88333333, 0.9       ]), 'split5_test_score': array([0.5       , 0.91666667, 0.93333333, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n",
      "       0.95      , 0.95      , 0.95      ]), 'split6_test_score': array([0.5       , 0.93333333, 0.95      , 0.93333333, 0.93333333,\n",
      "       0.93333333, 0.93333333, 0.93333333, 0.91666667, 0.93333333,\n",
      "       0.93333333, 0.93333333, 0.91666667]), 'split7_test_score': array([0.50847458, 0.93220339, 0.93220339, 0.88135593, 0.88135593,\n",
      "       0.88135593, 0.86440678, 0.86440678, 0.86440678, 0.86440678,\n",
      "       0.86440678, 0.86440678, 0.88135593]), 'split8_test_score': array([0.50847458, 0.91525424, 0.93220339, 0.91525424, 0.93220339,\n",
      "       0.93220339, 0.93220339, 0.93220339, 0.94915254, 0.94915254,\n",
      "       0.94915254, 0.94915254, 0.94915254]), 'split9_test_score': array([0.50847458, 0.91525424, 0.91525424, 0.89830508, 0.89830508,\n",
      "       0.89830508, 0.89830508, 0.89830508, 0.89830508, 0.89830508,\n",
      "       0.89830508, 0.89830508, 0.89830508]), 'mean_test_score': array([0.505     , 0.92666667, 0.93333333, 0.91166667, 0.91166667,\n",
      "       0.91166667, 0.91      , 0.90833333, 0.90666667, 0.90833333,\n",
      "       0.91      , 0.91166667, 0.91166667]), 'std_test_score': array([0.0040839 , 0.02255975, 0.02347277, 0.02791175, 0.02783574,\n",
      "       0.03145329, 0.03340383, 0.03344539, 0.03498045, 0.03580454,\n",
      "       0.0372867 , 0.04206528, 0.04621999]), 'rank_test_score': array([13,  2,  1,  3,  3,  3,  8, 10, 12, 10,  8,  3,  3], dtype=int32), 'split0_train_score': array([0.50463822, 0.94619666, 0.97773655, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split1_train_score': array([0.50463822, 0.94990724, 0.98330241, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split2_train_score': array([0.50463822, 0.95176252, 0.9851577 , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split3_train_score': array([0.50555556, 0.93703704, 0.98333333, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split4_train_score': array([0.50555556, 0.9462963 , 0.98888889, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split5_train_score': array([0.50555556, 0.94814815, 0.98518519, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split6_train_score': array([0.50555556, 0.94814815, 0.98333333, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split7_train_score': array([0.50462107, 0.95009242, 0.97966728, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split8_train_score': array([0.50462107, 0.93900185, 0.98336414, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'split9_train_score': array([0.50462107, 0.95933457, 0.98890943, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'mean_train_score': array([0.50500001, 0.94759249, 0.98388783, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]), 'std_train_score': array([0.00045365, 0.00596939, 0.00332274, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        ])}\n",
      "the error rate for the degree  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] is:  [0.495, 0.07333333333333336, 0.06666666666666665, 0.08833333333333337, 0.08833333333333337, 0.08833333333333337, 0.08999999999999997, 0.09166666666666667, 0.09333333333333338, 0.09166666666666667, 0.08999999999999997, 0.08833333333333337, 0.08833333333333337]\n",
      "the best is  {'degree': 2, 'kernel': 'poly'}\n",
      "poly-10cv has a test error rate of  0.06499999999999995\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#For the polynomial kernel, \n",
    "#tune the degree parameter using 10 fold cross validation. \n",
    "#Plot the cross-validation error rate v.s. degree parameters. \n",
    "#Fit the final model using the selected degree, and conduct prediction on the testing data. \n",
    "#Report testing error rate. \n",
    "\n",
    "\n",
    "\n",
    "g = list(range(13))\n",
    "#training with 10 fold cv\n",
    "param_grid = {'degree':g, 'kernel':['poly']}\n",
    "rsvm = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=10)\n",
    "rsvm.fit(X, y)\n",
    "\n",
    "#Plot the cross-validation error rate v.s. gamma parameters.\n",
    "res = rsvm.cv_results_\n",
    "print(res)\n",
    "re = list(map(lambda x:float(1.0 -x), res['mean_test_score']))\n",
    "rg = list(range(13)) #\n",
    "plot = plt.scatter(rg,re)\n",
    "print('the error rate for the degree ', g, 'is: ', re)\n",
    "\n",
    "\n",
    " \n",
    "#Fit the final model using the selected gamma, and conduct prediction on the testing data. \n",
    "print('the best is ', rsvm.best_params_)\n",
    "pred = 1 - rsvm.score(tX,ty)\n",
    "#Report testing error rate. \n",
    "print('poly-10cv has a test error rate of ', pred)\n",
    "te[2] = pred\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:458: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:463: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate for # trees  [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710, 720, 730, 740, 750, 760, 770, 780, 790, 800, 810, 820, 830, 840, 850, 860, 870, 880, 890, 900, 910, 920, 930, 940, 950, 960, 970, 980, 990, 1000] is:  [0.20333333333333337, 0.15000000000000002, 0.1383333333333333, 0.11166666666666669, 0.09166666666666667, 0.08833333333333337, 0.08833333333333337, 0.08333333333333337, 0.08499999999999996, 0.09166666666666667, 0.08333333333333337, 0.07166666666666666, 0.07333333333333336, 0.06833333333333336, 0.06666666666666665, 0.06499999999999995, 0.06999999999999995, 0.06666666666666665, 0.06333333333333335, 0.06333333333333335, 0.06166666666666665, 0.06333333333333335, 0.06833333333333336, 0.06499999999999995, 0.06833333333333336, 0.06833333333333336, 0.06999999999999995, 0.06666666666666665, 0.06666666666666665, 0.06833333333333336, 0.06333333333333335, 0.06000000000000005, 0.06333333333333335, 0.06333333333333335, 0.06499999999999995, 0.06666666666666665, 0.06666666666666665, 0.06499999999999995, 0.06166666666666665, 0.06333333333333335, 0.06499999999999995, 0.06499999999999995, 0.06333333333333335, 0.06333333333333335, 0.06333333333333335, 0.06666666666666665, 0.06499999999999995, 0.06666666666666665, 0.06666666666666665, 0.06666666666666665, 0.06666666666666665, 0.06666666666666665, 0.06166666666666665, 0.06333333333333335, 0.06333333333333335, 0.06666666666666665, 0.06499999999999995, 0.06499999999999995, 0.06666666666666665, 0.06833333333333336, 0.06833333333333336, 0.06333333333333335, 0.06499999999999995, 0.06333333333333335, 0.06499999999999995, 0.06499999999999995, 0.06166666666666665, 0.06499999999999995, 0.06499999999999995, 0.06499999999999995, 0.06499999999999995, 0.06166666666666665, 0.06166666666666665, 0.06166666666666665, 0.06333333333333335, 0.06333333333333335, 0.06499999999999995, 0.06333333333333335, 0.06333333333333335, 0.06333333333333335, 0.06333333333333335, 0.06333333333333335, 0.06333333333333335, 0.06666666666666665, 0.06499999999999995, 0.06166666666666665, 0.06333333333333335, 0.06333333333333335, 0.06166666666666665, 0.06000000000000005, 0.05666666666666664, 0.06000000000000005, 0.06000000000000005, 0.06333333333333335, 0.06166666666666665, 0.06000000000000005, 0.06166666666666665, 0.06166666666666665, 0.06000000000000005, 0.06000000000000005]\n",
      "stable error is 0.06000000000000005  for  320 trees\n",
      "random forest has a test error rate of  0.050000000000000044\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#(4) Run Random Forest on the training data. Keeping all other parameter at default, \n",
    "#fit the model using different number of trees (10, 20, 30,……, 1000). \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "g = list(range(10,1001,10))\n",
    "#training \n",
    "\n",
    "\n",
    "# NOTE: Setting the `warm_start` construction parameter to `True` disables\n",
    "# support for parallelized ensembles but is necessary for tracking the OOB\n",
    "# error trajectory during training.\n",
    "rf = RandomForestClassifier(n_estimators=1,warm_start=True, oob_score=True)\n",
    "\n",
    "# to store error\n",
    "error_rate = []\n",
    "\n",
    "\n",
    "for i in g:\n",
    "    rf.set_params(n_estimators=i)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    # Record the OOB error for each `n_estimators=i` setting.\n",
    "    oob_error = 1 - rf.oob_score_\n",
    "    error_rate.append(oob_error)\n",
    "\n",
    "\n",
    "\n",
    "#Plot the OOB error rate v.s. the number of trees. \n",
    "plot = plt.scatter(g,error_rate)\n",
    "print('the error rate for # trees ', g, 'is: ', error_rate)\n",
    "\n",
    "#Select the number of trees based on when the OOB error rate first stabilizes \n",
    "#(close enough to the error rate at 1000 trees, with a difference within 1%).\n",
    "#the error at 1000's lower and upper bounds\n",
    "le = error_rate[len(g)-1]*1.01\n",
    "ue  = error_rate[len(g)-1]*0.99\n",
    "j =0\n",
    "for i in error_rate:\n",
    "    \n",
    "    if (i<=le):\n",
    "        print( 'stable error is', i, ' for ',g[j],'trees')\n",
    "        break\n",
    "    j=j+1\n",
    "\n",
    "\n",
    "\n",
    "#Select the number of trees based on when the OOB error rate first stabilizes \n",
    "# and conduct prediction on the testing data. \n",
    "#train\n",
    "rf = RandomForestClassifier(n_estimators=680,warm_start=True, oob_score=True)\n",
    "rf.fit(X, y)\n",
    "#predict\n",
    "pred = 1 - rf.score(tX,ty)\n",
    "#Report testing error rate. \n",
    "print('random forest has a test error rate of ', pred)\n",
    "te[3] = pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of predictors based on importance score:\n",
      "1. feature 7 (0.297669)\n",
      "2. feature 1 (0.245494)\n",
      "3. feature 5 (0.037635)\n",
      "4. feature 21 (0.023690)\n",
      "5. feature 20 (0.022747)\n",
      "6. feature 11 (0.022371)\n",
      "7. feature 22 (0.021630)\n",
      "8. feature 10 (0.021381)\n",
      "9. feature 2 (0.020894)\n",
      "10. feature 9 (0.020565)\n",
      "11. feature 3 (0.019165)\n",
      "12. feature 8 (0.019137)\n",
      "13. feature 17 (0.019008)\n",
      "14. feature 14 (0.018808)\n",
      "15. feature 24 (0.018663)\n",
      "16. feature 15 (0.018611)\n",
      "17. feature 16 (0.018535)\n",
      "18. feature 19 (0.018336)\n",
      "19. feature 0 (0.017693)\n",
      "20. feature 23 (0.017472)\n",
      "21. feature 13 (0.017186)\n",
      "22. feature 4 (0.016689)\n",
      "23. feature 18 (0.016092)\n",
      "24. feature 6 (0.015498)\n",
      "25. feature 12 (0.015032)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Rank the predictors based on their importance score. \n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Rank of predictors based on importance score:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached https://files.pythonhosted.org/packages/6a/49/7e10686647f741bd9c8918b0decdb94135b542fe372ca1100739b8529503/xgboost-0.82-py2.py3-none-manylinux1_x86_64.whl\n",
      "Collecting numpy (from xgboost)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting scipy (from xgboost)\n",
      "  Using cached https://files.pythonhosted.org/packages/7f/5f/c48860704092933bf1c4c1574a8de1ffd16bf4fde8bab190d747598844b2/scipy-1.2.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Installing collected packages: numpy, scipy, xgboost\n",
      "Successfully installed numpy-1.16.2 scipy-1.2.1 xgboost-0.82\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the gb treee is:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "the XGB gradient boosted tree has a test error rate of  0.05500000000000005 \n",
      "\n",
      "with feature importance  [0.01786581 0.24626149 0.02424451 0.03576612 0.01582746 0.05910151\n",
      " 0.01855204 0.24128076 0.02423516 0.0006017  0.02172776 0.02791414\n",
      " 0.03062525 0.01270761 0.01083593 0.02823354 0.01190676 0.02502947\n",
      " 0.00667074 0.02407626 0.01209118 0.02648786 0.03783779 0.01901644\n",
      " 0.02110271]\n",
      "\n",
      "to compare, the error for ['radial', 'sigmoid', 'polynomial', 'rand forest']  are  [0.07999999999999996, 0.385, 0.06499999999999995, 0.050000000000000044]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#(5) Run gradient boosting of trees at default setting on the training data. \n",
    "gbt = xgb.XGBClassifier()\n",
    "gbt.fit(X, y)\n",
    "print('the gb treee is: ', gbt)\n",
    "#Predict the testing data. Report the testing data error rate.\n",
    "pred = 1 - gbt.score(tX,ty)\n",
    "#Report testing error rate. \n",
    "print('the XGB gradient boosted tree has a test error rate of ', pred, '\\n')\n",
    "\n",
    "#Report variable importance. \n",
    "print('with feature importance ', gbt.feature_importances_)\n",
    "print('\\nto compare, the error for', ln, ' are ', te)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
